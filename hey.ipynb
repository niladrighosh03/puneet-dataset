{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae349ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 19:43:10.724272: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 19:43:10.739536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-03 19:43:10.757125: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-03 19:43:10.762256: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-03 19:43:10.775413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 19:43:13.388153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9bf1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121f8550262e4b34b03f64280fe4d841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… Load Qwen model\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec660eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# LLM PROMPT FUNCTION\n",
    "# =============================\n",
    "\n",
    "def ask_llm(question, options, answer):\n",
    "    prompt = \"\"\"\n",
    "You are an evaluator responsible for checking the quality of multiple-choice questions (MCQs).\n",
    "\n",
    "Task: Assess the following dataset entry:\n",
    "\n",
    "- Question: {question}\n",
    "- Options: {options}\n",
    "- Correct Answer: {answer}\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Is the question factually accurate and logically correct?\n",
    "2. Is the labeled correct answer actually correct?\n",
    "3. Does the question function well as a proper MCQ? (Clear wording, non-overlapping options, etc.)\n",
    "\n",
    "Response Format (strictly follow this):\n",
    "Score: 1 or 0\n",
    "Mistake: (brief explanation or \"no mistake\")\n",
    "Improvement: (suggest correction or \"no correction needed\")\n",
    "\n",
    "Few-Shot Examples:\n",
    "\n",
    "Example 1:\n",
    "Question: Where did Boli Khela originate?\n",
    "Options: A) India, B) Bangladesh, C) Nepal, D) Sri Lanka\n",
    "Correct Answer: B) Bangladesh\n",
    "Expected Output:\n",
    "Score: 1\n",
    "Mistake: no mistake\n",
    "Improvement: no correction needed\n",
    "\n",
    "Example 2:\n",
    "Question: What is the primary objective in Boli Khela?\n",
    "Options: A) Running, B) Wrestling, C) Throwing a ball, D) Kicking a ball\n",
    "Correct Answer: B) Wrestling\n",
    "Expected Output:\n",
    "Score: 1\n",
    "Mistake: no mistake\n",
    "Improvement: no correction needed\n",
    "\n",
    "Example 3:\n",
    "Question: When is Boli Khela traditionally played?\n",
    "Options: A) Winter, B) Summer, C) During festivals, D) Spring\n",
    "Correct Answer: C) During festivals\n",
    "Expected Output:\n",
    "Score: 1\n",
    "Mistake: no mistake\n",
    "Improvement: no correction needed\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    out = llm(prompt, max_length=350, do_sample=False)\n",
    "    return out[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888e236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/DATA/rohan_kirti/niladri/pks/\"  # main folder containing all countries\n",
    "output_file = \"/DATA/rohan_kirti/niladri/pks/final_llm_results.csv\"\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "# âœ… Regional language column mapping (update if needed)\n",
    "COL_QUESTION = \"Question\"\n",
    "COL_OPTIONS  = \"Options\"\n",
    "COL_ANSWER   = \"Answer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97d3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… If output file doesnâ€™t exist â†’ write header\n",
    "import csv\n",
    "if not os.path.exists(output_file):\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"country\", \"file\", \"sheet\",\n",
    "                         \"question\", \"option\", \"answer\", \"llm_output\"])\n",
    "\n",
    "# âœ… Load already processed rows (to skip on restart)\n",
    "processed = set()\n",
    "df_existing = pd.read_csv(output_file, encoding=\"utf-8-sig\")\n",
    "for i, row in df_existing.iterrows():\n",
    "    processed.add((row[\"file\"], row[\"sheet\"], row[\"question\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d3b68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=350) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Processing: /DATA/rohan_kirti/niladri/pks/Bangladesh_HBQ.xlsx\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================\n",
    "# MAIN PROCESS\n",
    "# =============================\n",
    "i=1\n",
    "excel_files = glob.glob(os.path.join(base_path, \"**/*.xlsx\"), recursive=True)\n",
    "\n",
    "for file in excel_files:\n",
    "    if i>11:\n",
    "        break\n",
    "    i+=1\n",
    "    country = os.path.basename(os.path.dirname(file))\n",
    "    xls = pd.ExcelFile(file)\n",
    "    print(f\"\\nðŸ“Œ Processing: {file}\")\n",
    "\n",
    "    for sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(file, sheet_name=sheet)\n",
    "\n",
    "        if COL_QUESTION not in df.columns:\n",
    "            continue\n",
    "\n",
    "        df = df[[COL_QUESTION, COL_OPTIONS, COL_ANSWER]].dropna(how=\"all\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            key = (os.path.basename(file), sheet, row[COL_QUESTION])\n",
    "\n",
    "            if key in processed:\n",
    "                continue\n",
    "\n",
    "            q, option, ans = row[COL_QUESTION], row[COL_OPTIONS], row[COL_ANSWER]\n",
    "\n",
    "            try:\n",
    "                llm_response = ask_llm(q, option, ans)\n",
    "            except Exception as e:\n",
    "                llm_response = f\"ERROR: {e}\"\n",
    "\n",
    "            # âœ… WRITE IMMEDIATELY to CSV\n",
    "            with open(output_file, \"a\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([country, os.path.basename(file), sheet,\n",
    "                                 q, option, ans, llm_response])\n",
    "\n",
    "            processed.add(key)\n",
    "\n",
    "            print(f\"âœ… Saved â†’ {country} | {sheet} | Row {idx}\")\n",
    "            # time.sleep(0.3)  # prevent overload\n",
    "\n",
    "print(\"\\nâœ… COMPLETE!\")\n",
    "print(\"ðŸ“„ Live-updating CSV:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b48f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# SAVE RESULTS\n",
    "# =============================\n",
    "\n",
    "df_out = pd.DataFrame(processed_data)\n",
    "df_out.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nâœ… DONE! Results saved to:\", output_file)\n",
    "print(f\"âœ… Total responses generated: {len(df_out)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
